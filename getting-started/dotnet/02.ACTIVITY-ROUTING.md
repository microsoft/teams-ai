# Activity Routing


When an incoming activity reaches the server, the bot adapter handles the necessary authentication and creates a `TurnContext` object that encapsulates the activity details. It then calls the `OnTurnAsync` method. This is the entry point method of the application. Here's what happens in this method:

1. If configured in the application options, pulses of the `Typing` activity are sent to the user.
2. If configured in the application options, the @mention is removed from the incoming message activity.
3. The turn state is loaded using the configured turn state manager.
4. The `OnBeforeTurnAsync` activity handler is executed. If it returns false, save turn state to storage.
5. All text-based messages are handled through `OnMessageActivityAsync`. If there is an AI setup and `OnMessageActvityAsync` throws a `NotImplementedException`, then `ChainAsync` is called and executed.
6. The `AfterTurnAsync` activity handler is executed. If it return true, save turn state to storage.

These six steps happen every time an incoming activity is received by the server.


If you are familiar with botbuilder, you already know the basics of turn flow in Teams AI. The main differences are `BeforeTurn`, `AfterTurn`, and how AI fits into the system.

Here's a diagram of the turn flow:
![diagram of Teams AI application flow](../assets/image.png)

<!-- ```dotnet
[Action("ActionName")]
public async Task<bool> ActionName([ActionTurnContext] ITurnContext turnContext, [ActionTurnState] AppState turnState) { ... } 
```-->

<!-- The above is the setup for a specific action.

1. The AI module generates a plan of actions on what to do with the response that was generated. (More details provided in <a href="#what-ties-the-ai-module-together-action-mapping">AI Module</a> section)
2. The module will execute those actions, and fill its response (`$output`) with the appropriate data.
3. The response is passed to `AfterTurn`, in `TempState` includes:
   - `$history` - AI or conversation history, the length (number of turns) of which is determined by the prompt configuration
   - `$input` - the user's original input
   - `$output` - the generated response by the AI
4. `AfterTurn` is intended to be used for cleanup after the response has been sent.

Note that Bot memory and AI memory are distinct.

### Bot memory

> Bot memory and AI memory are separate. Bot memory stores data for the bot to use, while AI memory stores AI data.

- `$conversation.<prop>` - bot conversation memory
- `$user.<prop>` - bot user memory
- `$temp.<prop>` - bot temp memory (data kept for 1 turn only)
  - `$<prop>` may be used as an alias for `$temp.<prop>`

## AI memory (`$history`)

> AI bot memory may store context/information for as little as 1 turn, while bot memory may be used to store information for the lifetime of the conversation.

> Unlike bot memory, AI memory consumes tokens and therefore is more expensive, but keeping a shorter `$history` may cause more frequent hallucinations from the AI.

- `$history` - conversation history tracked by AI (not related to bot's conversation history see [Bot memory](#bot-memory))
- `$input` - input from the prompt, such as `activity.text`
- `$output` - the last executed function's output. You reference this output in code as `state.temp.value.output`
- Hallucinatiion - when the AI creates an independant context/response that does not match the app's use-cases.
- Parent prompt: when executing chaining, the output from the child prompt may be directly passed to the parent prompt:

## What ties the AI module together: Action Mapping

`ChainAsync` is utilized when the Planner recieves an input that did not trigger an activity handler, and instead generates and implements a plan. That plan is the list of directions that the LLM will follow according to the predicted action from the user's input. For example, in action mapping LightBot, the following excerpt is a section of the plan:

### DO and SAY

```txt
The assistant must return the following JSON structure:

{"type":"plan","commands":[{"type":"DO","action":"<name>","entities":{"<name>":<value>}},{"type":"SAY","response":"<response>"}]}

The following actions are supported:

- LightsOn
- LightsOff
- Pause time=<duration in ms>
- LightStatus
```

The first two lines of directions asserts that the AI _must_ return its response in JSON format. This means that `$output` will return JSON, and therefore be parsable in a predictable way.

- The JSON provided is minified to reduce token ussage
- There are two types of commands: `DO` and `SAY`

The commands are provided as an array to perform in order.

- `DO` is an action defined by your code in `LightBotActions.cs`. These are programmatic methods not defined in natural language, but in your code.
  - In the prompt above, the performable `DO` actions are `LightsOn`, `LightsOff`, `Pause`, and `LightStatus`
- `SAY` is a response to add to the conversation, exactly like the response from a bot. -->
