# Concepts

See below to read more details about new concepts.

## How an incoming activity is routed in the Application:

When an incoming activity reaches the server, the bot adapter handles the necessary authentication and creates a `TurnContext` object that encapsulates the activity details. It then calls the `OnTurnAsync` method. This is the entry point method of the application. Here's what happens in this method:

1. If configured in the application options, pulses of the `Typing` activity is sent to the user.
2. If configured in the application options, the @mention is removed from the incoming message activity.
3. The turn state is loaded using the configured turn state manager.
4. The `OnBeforeTurnAsync` activity handler is executed. If it returns false, save turn state to storage.
5. If the corresponding activity handler is configured, then it is executed. Otheriwse the AI module's `ChainAsync` executes.
6. The `AfterTurnAsync` activity handler is executed. If it return true, save turn state to storage.

These six steps happen every time an incoming activity is received by the server.

Here's a high level overview:

![diagram of Teams AI application flow](../assets/image.png)

1. The change in flow begins when an activity handler is not registered with the app. If that is the case, the turn context and turn state is passed to the AI module.
1. The AI module will determine its response via the prompt (which may include conversation history)
   - `app.ai.action('___', ...)` is the code for a specific action.
1. The AI module generates a plan of actions on what to do with the response that was generated. (More details provided in <a href="#what-ties-the-ai-module-together-action-mapping> section)
1. The module will execute those actions, and fill its response (`$output`) with the appropriate data.
1. The response is passed to `AfterTurn`, which includes:
   - `$history` - conversation history, the length (number of turns) of which is determined by the prompt configuration
   - `$input` - the user's original input
   - `$output` - the generated response by the AI
1. `AfterTurn` is intended to be used for cleanup after the response has been sent.

## What ties the AI module together: Action Mapping

`ChainAsync` is utilized when the Planner recieves an input that did not trigger an activity handler, and instead generates and implements a plan. That plan is the list of directions that the LLM will follow according to the predicted action from the user's input. For example, in action mapping LightBot, the following excerpt is a section of the plan:

### Excerpt of LightBot prompt

```txt
The assistant must return the following JSON structure:

{"type":"plan","commands":[{"type":"DO","action":"<name>","entities":{"<name>":<value>}},{"type":"SAY","response":"<response>"}]}

The following actions are supported:

- LightsOn
- LightsOff
- Pause time=<duration in ms>
- LightStatus
```

### Description of LightBot prompt excerpt.

1. The first two lines of directions asserts that the AI _must_ return its response in JSON format. This means that `$output` will return JSON, and therefore be parsable in a predictable way.
   - The JSON provided is minified to reduce token ussage
   - There are two types of commands: `DO` and `SAY`

The commands are provided as an array to perform in order.

- `DO` is an action defined by your code in `LightBotActions.cs`. These are programmatic methods not defined in natural language, but in your code.
  - In the prompt above, the performable `DO` actions are `LightsOn`, `LightsOff`, `Pause`, and `LightStatus`
- `SAY` is a response to add to the conversation, exactly like the response from a bot. At this point, the content

4. Show the custom user registered actions executing.
